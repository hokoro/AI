{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5ec833f",
   "metadata": {},
   "source": [
    "### 나이브 베이즈 분류기 - 트윗."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cebe945",
   "metadata": {},
   "source": [
    "**<span style=\"color:blue\">목표</span>**: 베이즈 정리를 적용하여 자연어 분류 모형을 만들어 본다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ef8a1d",
   "metadata": {},
   "source": [
    "<br>\n",
    "1. 베이즈 정리. <br>\n",
    "\n",
    " $P(A| W_1, W_2, W_3,...) = {P(W_1, W_2, W_3,...|A) \\cdot P(A) \\over P(W_1, W_2, W_3,....)}$ <br>\n",
    " $P(B| W_1, W_2, W_3,...) = {P(W_1, W_2, W_3,...|B) \\cdot P(B) \\over P(W_1, W_2, W_3,....)}$\n",
    "\n",
    "<br>\n",
    "2. 동일한 분모를 무시하면 다음 비례관계가 성립된다. <br>\n",
    "\n",
    " $P(A| W_1, W_2, W_3, \\ldots) \\sim P(W_1, W_2, W_3,...|A) \\cdot P(A) $ <br>\n",
    " $P(B| W_1, W_2, W_3, \\ldots) \\sim P(W_1, W_2, W_3,...|B) \\cdot P(B) $\n",
    " \n",
    "<br>\n",
    "3. 독립적인 단어 분포를 전제하면 다음과 같이 분해할 수 있다. <br>\n",
    "\n",
    " $P(A| W_1, W_2, W_3, \\ldots) \\sim  P(W_1|A)\\cdot P(W_2|A)\\cdot P(W_3|A) \\cdots P(A) $ <br>\n",
    " $P(B| W_1, W_2, W_3, \\ldots) \\sim P(W_1|B)\\cdot P(W_2|B)\\cdot P(W_3|B) \\cdots P(B) $\n",
    "\n",
    "<br>\n",
    "4. 이제는 로그함수를 적용해 본다. <br>\n",
    "\n",
    "$Log_A \\sim  Log(P(W_1|A)) +  Log(P(W_2|A)) + Log(P(W_3|A)) + \\ldots + Log(P(A)) $ <br>\n",
    "$Log_B \\sim  Log(P(W_1|B)) +  Log(P(W_2|B)) + Log(P(W_3|B)) + \\ldots + Log(P(B)) $\n",
    "<br>\n",
    "\n",
    "**<span style=\"color:blue\">결론</span>**:  $Log_A$와 $Log_B$를 비교해서 큰 쪽으로 인식!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395f8367",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3170ba18",
   "metadata": {},
   "source": [
    "#### 1. 학습 자료를 읽어와서 전처리한다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7c606f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A 유형의 트윗을 읽어온다. \n",
    "f = open(\"../data/tweets_A.txt\",\"r\")\n",
    "ta = f.readlines()\n",
    "f.close()\n",
    "\n",
    "# B 유형의 트윗을 읽어온다. \n",
    "f = open(\"../data/tweets_B.txt\",\"r\")\n",
    "tb = f.readlines()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a43ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"A 유형의 수 : {len(ta)}\")\n",
    "print(f\"B 유형의 수 : {len(tb)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68005f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리 해서 단어별 도수표를 만들어주는 함수.\n",
    "def preprocessor(tweets):\n",
    "    freq_dict = {}\n",
    "    for a_line in tweets:\n",
    "        a_line = a_line.lower()               # 소문자화.\n",
    "        a_line = re.sub(r\"\\W\",\" \",a_line)     # 특수문자 제거.\n",
    "        a_line = re.sub(r\"\\d\", \" \", a_line)   # 숫자 제거.\n",
    "        a_line = re.sub(\"a|the|and|or|because|at\", \" \",a_line)  #  불용어 제거.\n",
    "        a_line = a_line.split()               # 분절.\n",
    "        for a_word in a_line:\n",
    "            if len(a_word) > 3:               # 길이가 최소 조건을 충족하는 단어만 사용.\n",
    "                if a_word in freq_dict:\n",
    "                    freq_dict[a_word] += 1    # 카운트 누적.\n",
    "                else:\n",
    "                    freq_dict[a_word] = 2     # 처음 발견. 기본값 1 + 누적 1.\n",
    "    return freq_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436c4253",
   "metadata": {},
   "source": [
    "#### 2. 학습 모형을 준비해 둔다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1acb1bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Series로 변환.\n",
    "freq_a = pd.Series(preprocessor(ta)).sort_values(ascending=False)\n",
    "freq_b = pd.Series(preprocessor(tb)).sort_values(ascending=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fcd1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocabulary 사전 크기 확인.\n",
    "print(\"Size of A: {}\".format( len(freq_a)))\n",
    "print(\"Size of B: {}\".format( len(freq_b)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c8716f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocabulary 수 맞춤.\n",
    "n_voca = 300\n",
    "freq_a = freq_a.iloc[:n_voca]\n",
    "freq_b = freq_b.iloc[:n_voca]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e0a3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 로그 확률로 변환.\n",
    "freq_a_sum = freq_a.sum()\n",
    "freq_b_sum = freq_b.sum()\n",
    "log_prob_a = dict(np.log(freq_a/freq_a_sum))    # Log(P(W|A))\n",
    "log_prob_b = dict(np.log(freq_b/freq_b_sum))    # Log(P(W|B))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527354d9",
   "metadata": {},
   "source": [
    "#### 3. 테스트 데이터를 읽어온다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4120713",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 트윗을 읽어온다 (X_test). \n",
    "f = open(\"../data/tweets_test.txt\",\"r\")\n",
    "tt = f.readlines()\n",
    "f.close()\n",
    "\n",
    "# 테스트 트윗의 유형 정보를 읽어온다 (Y_test).\n",
    "f = open(\"../data/tweets_test_class.txt\",\"r\")\n",
    "Y_test_raw = f.read()                 # 한 덩어리로 읽어온다.\n",
    "Y_test = Y_test_raw.split()           # 분절을 통해서 깔끔히!\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5536f16",
   "metadata": {},
   "source": [
    "#### 4. 예측을 실시한다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f65107",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = []\n",
    "for a_sentence in tt:\n",
    "    log_prob_sum_a = 0.\n",
    "    log_prob_sum_b = 0.\n",
    "    a_sent_preprocessed = preprocessor([a_sentence])\n",
    "    for a_word, a_freq in a_sent_preprocessed.items():\n",
    "        if a_word in log_prob_a:\n",
    "            log_prob_sum_a += log_prob_a[a_word]*a_freq\n",
    "        else:\n",
    "            log_prob_sum_a += np.log(1.0/freq_a_sum)*a_freq\n",
    "            \n",
    "        if a_word in log_prob_b:\n",
    "            log_prob_sum_b += log_prob_b[a_word]*a_freq\n",
    "        else:\n",
    "            log_prob_sum_b += np.log(1.0/freq_b_sum)*a_freq\n",
    "            \n",
    "    if (log_prob_sum_a > log_prob_sum_b):\n",
    "        Y_pred.append(\"A\")\n",
    "    else:\n",
    "        Y_pred.append(\"B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad5b9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정확도 계산.\n",
    "correct = pd.Series([ x == y for (x,y) in zip(Y_pred, Y_test) ])\n",
    "print(\"Accuracy : {}\".format(correct.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f71adc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
